{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : CTC Loss\n",
    "from typing import List\n",
    "\n",
    "import cv2\n",
    "import gdown\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "\n",
    "#import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [46:08,  2.77s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keypoints extraction and saving to CSV completed.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "# Define paths\n",
    "data_path = 'data3/s3'\n",
    "\n",
    "# Initialize dlib's face detector (HOG-based) and then create the facial landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"data_kaggle/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Function to extract keypoints\n",
    "def extract_keypoints_from_frame(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    rects = detector(gray, 1)\n",
    "    keypoints = []\n",
    "    for rect in rects:\n",
    "        shape = predictor(gray, rect)\n",
    "        for i in range(0, 68):\n",
    "            keypoints.append((shape.part(i).x, shape.part(i).y))\n",
    "    return keypoints\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "# Iterate through videos\n",
    "for root, dirs, files in os.walk(data_path):\n",
    "    for file in tqdm(files, total=1000):\n",
    "        if file.endswith(\".mpg\"):  # Add other video formats if needed\n",
    "            video_path = os.path.join(root, file)\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            frame_number = 0\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                keypoints = extract_keypoints_from_frame(frame)\n",
    "                for idx, (x, y) in enumerate(keypoints):\n",
    "                    results.append({'video_path': video_path, 'frame_number': frame_number, 'x': x, 'y': y, 'point_index': idx})\n",
    "                frame_number += 1\n",
    "            cap.release()\n",
    "\n",
    "# Convert list to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df.to_csv('facial_keypoints.csv', index=False)\n",
    "\n",
    "print(\"Keypoints extraction and saving to CSV completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"facial_keypoints.csv\")\n",
    "video_path_counts = df.video_path.value_counts()\n",
    "video_paths_to_keep = video_path_counts[video_path_counts == 5100].index\n",
    "df = df[df['video_path'].isin(video_paths_to_keep)]\n",
    "\n",
    "df_filtered = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (986, 75, 68, 2)\n"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "\n",
    "num_frames_per_chunk = 5100\n",
    "frames_per_video = 75\n",
    "keypoints_per_frame = 68\n",
    "coords_per_keypoint = 2\n",
    "\n",
    "for video_path, group in df_filtered.groupby('video_path'):\n",
    "    num_frames = len(group) // keypoints_per_frame\n",
    "    num_chunks = num_frames // frames_per_video\n",
    "    for i in range(num_chunks):\n",
    "        chunk = group.iloc[i * frames_per_video * keypoints_per_frame:(i + 1) * frames_per_video * keypoints_per_frame]\n",
    "        chunk_reshaped = chunk[['x', 'y']].values.reshape(-1, keypoints_per_frame, coords_per_keypoint)\n",
    "        chunks.append(chunk_reshaped)\n",
    "\n",
    "chunks_array = np.array(chunks)\n",
    "print(f\"Data shape: {chunks_array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(986, 75, 68, 2)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab():\n",
    "    vocab = \"abcdefghijklmnopqrstuvwxyz123456789 \"\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def char_to_int(char):\n",
    "    # shift 1 \n",
    "    vocab = \"abcdefghijklmnopqrstuvwxyz123456789 \"\n",
    "    return vocab.index(char) + 1 if char in vocab else -1\n",
    "\n",
    "def int_to_char(index):\n",
    "    # shift 1 \n",
    "    vocab = \"abcdefghijklmnopqrstuvwxyz123456789 \"\n",
    "    return vocab[index - 1] if 1 <= index <= len(vocab) else ''\n",
    "\n",
    "def load_alignments(path:str) -> List[str]:\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    tokens = []\n",
    "    for line in lines:\n",
    "        line = line.split()\n",
    "        if line[2] != 'sil':\n",
    "            tokens.extend([*line[2]])\n",
    "            tokens.append(' ')\n",
    "    return [char_to_int(token) for token in tokens]\n",
    "\n",
    "all_alignments = []\n",
    "for video_path in df['video_path'].unique():\n",
    "    datapath = video_path.split('/')[0]\n",
    "    speaker_path = video_path.split('/')[-1].split('\\\\')[0]\n",
    "    vid_path = video_path.split('/')[-1].split('\\\\')[-1].split('.')[0]\n",
    "    \n",
    "    alignment_path = os.path.join(f'{datapath}','align',f'{vid_path}.align')\n",
    "    alignments = load_alignments(alignment_path) \n",
    "    all_alignments.append(alignments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(len(label) for label in all_alignments)\n",
    "label_data = [np.pad(label, (0, max_len - len(label)), 'constant', constant_values=0) for label in all_alignments]\n",
    "\n",
    "# Convert label_data to numpy array\n",
    "label_data = np.array(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  9, 14, ...,  0,  0,  0],\n",
       "       [ 2,  9, 14, ...,  0,  0,  0],\n",
       "       [ 2,  9, 14, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [19,  5, 20, ...,  0,  0,  0],\n",
       "       [19,  5, 20, ...,  0,  0,  0],\n",
       "       [19,  5, 20, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([986, 75, 68, 2])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data_tensor = torch.tensor(chunks_array, dtype=torch.float32)\n",
    "input_data_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([986, 36])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_data_tensor = torch.tensor(label_data, dtype=torch.long)\n",
    "label_data_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(input_data_tensor, label_data_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = create_vocab()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 3060\n",
      "Epoch [1/25], Training Loss: 3.9520, Validation Loss: 3.3297\n",
      "Epoch [2/25], Training Loss: 3.1447, Validation Loss: 3.0478\n",
      "Epoch [3/25], Training Loss: 2.9758, Validation Loss: 2.8821\n",
      "Epoch [4/25], Training Loss: 2.8342, Validation Loss: 2.7030\n",
      "Epoch [5/25], Training Loss: 2.6812, Validation Loss: 2.6743\n",
      "Epoch [6/25], Training Loss: 2.6079, Validation Loss: 2.5546\n",
      "Epoch [7/25], Training Loss: 2.5213, Validation Loss: 2.4604\n",
      "Epoch [8/25], Training Loss: 2.4378, Validation Loss: 2.3652\n",
      "Epoch [9/25], Training Loss: 2.3349, Validation Loss: 2.2781\n",
      "Epoch [10/25], Training Loss: 2.2343, Validation Loss: 2.1683\n",
      "Epoch [11/25], Training Loss: 2.1591, Validation Loss: 2.0926\n",
      "Epoch [12/25], Training Loss: 2.0912, Validation Loss: 2.0195\n",
      "Epoch [13/25], Training Loss: 2.0261, Validation Loss: 1.9711\n",
      "Epoch [14/25], Training Loss: 1.9563, Validation Loss: 1.8845\n",
      "Epoch [15/25], Training Loss: 1.8975, Validation Loss: 1.8478\n",
      "Epoch [16/25], Training Loss: 1.8485, Validation Loss: 1.7998\n",
      "Epoch [17/25], Training Loss: 1.8064, Validation Loss: 1.8354\n",
      "Epoch [18/25], Training Loss: 1.7937, Validation Loss: 1.7279\n",
      "Epoch [19/25], Training Loss: 1.8279, Validation Loss: 1.7702\n",
      "Epoch [20/25], Training Loss: 1.7538, Validation Loss: 1.7214\n",
      "Epoch [21/25], Training Loss: 1.7098, Validation Loss: 1.6492\n",
      "Epoch [22/25], Training Loss: 1.6818, Validation Loss: 1.6594\n",
      "Epoch [23/25], Training Loss: 1.6712, Validation Loss: 1.6415\n",
      "Epoch [24/25], Training Loss: 1.6394, Validation Loss: 1.5936\n",
      "Epoch [25/25], Training Loss: 1.6287, Validation Loss: 1.6054\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "class LipReadingRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LipReadingRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_size)\n",
    "        )\n",
    "        self.bilstm = nn.LSTM(hidden_size, hidden_size, num_layers=4, batch_first=True, bidirectional=True, dropout=0.3)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, num_layers=4, batch_first=True, dropout=0.3)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for layer in self.input_layer:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "        \n",
    "        for name, param in self.bilstm.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "                \n",
    "        for layer in self.output_layer:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        \n",
    "        h0 = torch.zeros(8, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(8, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.bilstm(x, (h0, c0))\n",
    "        out = self.gru(out)[0]\n",
    "        out = self.dropout(out)\n",
    "        out = self.output_layer(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "input_size = 68 * 2\n",
    "hidden_size = 512\n",
    "num_classes = len(vocab) + 1\n",
    "model = LipReadingRNN(input_size, hidden_size, num_classes).to(device)\n",
    "criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = StepLR(optimizer, step_size=200, gamma=0.1)\n",
    "\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "\n",
    "num_epochs = 25\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for sequences, labels in train_loader:\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        sequences = sequences.view(sequences.size(0), sequences.size(1), -1).to(device) # (batch_size, seq_len, input_size)\n",
    "        \n",
    "        outputs = model(sequences)\n",
    "        \n",
    "        outputs = outputs.permute(1, 0, 2)\n",
    "        input_lengths = torch.full((sequences.size(0),), sequences.size(1), dtype=torch.long).to(device)\n",
    "        target_lengths = torch.tensor([label[label != 0].size(0) for label in labels], dtype=torch.long).to(device)\n",
    "        loss = criterion(outputs.log_softmax(2), labels, input_lengths, target_lengths).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in test_loader:\n",
    "            sequences = sequences.to(device)\n",
    "            labels = labels.to(device)\n",
    "            sequences = sequences.view(sequences.size(0), sequences.size(1), -1).to(device) # (batch_size, seq_len, input_size)\n",
    "\n",
    "            outputs = model(sequences)\n",
    "\n",
    "            outputs = outputs.permute(1, 0, 2)\n",
    "            input_lengths = torch.full((sequences.size(0),), sequences.size(1), dtype=torch.long).to(device)\n",
    "            target_lengths = torch.tensor([label[label != 0].size(0) for label in labels], dtype=torch.long).to(device)\n",
    "            loss = criterion(outputs.log_softmax(2), labels, input_lengths, target_lengths).to(device)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {total_loss/len(train_loader):.4f}, Validation Loss: {val_loss/len(test_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch, sequences shape: torch.Size([32, 75, 136])\n",
      "Golden: place white by x seven soon \n",
      "Predicted: sla bree at ie no\n",
      "Golden: bin green by b zero please \n",
      "Predicted: sla bree at ie no\n",
      "Golden: set green by p five sp again \n",
      "Predicted: sla bree at ie no\n",
      "Golden: set white with v seven again \n",
      "Predicted: sla bree at ie no\n",
      "Golden: bin blue by z eight please \n",
      "Predicted: sla bree at ie no\n",
      "Golden: lay green in r six now \n",
      "Predicted: sla bree at ie no\n",
      "Golden: place red by p seven soon \n",
      "Predicted: sla bree at ie no\n",
      "Golden: set red with sp b nine again \n",
      "Predicted: sla bree at ie no\n",
      "Golden: place blue at v three again \n",
      "Predicted: sla bree at ie no\n",
      "Golden: lay green with t zero please \n",
      "Predicted: sla bree at ie no\n",
      "Golden: set white with v four now \n",
      "Predicted: sla bree at ie no\n",
      "Golden: place white by j nine soon \n",
      "Predicted: sla bree at ie no\n",
      "Golden: set white sp at b five soon \n",
      "Predicted: sla bree at ie no\n",
      "Golden: set white in n nine sp soon \n",
      "Predicted: sla bree at ie no\n",
      "Golden: bin green with o two please \n",
      "Predicted: sla bree at ie no\n",
      "Golden: bin green at a five soon \n",
      "Predicted: sla bree at ie no\n",
      "Golden: bin red with g six please \n",
      "Predicted: sla bree at ie no\n",
      "Golden: set white by v one soon \n",
      "Predicted: sla bree at ie no\n",
      "Golden: lay blue in x two now \n",
      "Predicted: sla bree at ie no\n",
      "Golden: place sp blue in h nine soon \n",
      "Predicted: sla bree at ie no\n",
      "Golden: lay red in q five soon \n",
      "Predicted: sla bree at ie no\n",
      "Golden: set blue at t seven again \n",
      "Predicted: sla bree at ie no\n",
      "Golden: bin green by u one soon \n",
      "Predicted: sla bree at ie no\n",
      "Golden: place green by e three again \n",
      "Predicted: sla bree at ie no\n",
      "Golden: bin green by n six now \n",
      "Predicted: sla bree at ie no\n",
      "Golden: set white at u eight please \n",
      "Predicted: sla bree at ie no\n",
      "Golden: bin green with h eight please \n",
      "Predicted: sla bree at ie no\n",
      "Golden: place red with x four now \n",
      "Predicted: sla bree at ie no\n",
      "Golden: set red by b three soon \n",
      "Predicted: sla bree at ie no\n",
      "Golden: lay blue with e three soon \n",
      "Predicted: sla bree at ie no\n",
      "Golden: place red in o eight now \n",
      "Predicted: sla bree at ie no\n",
      "Golden: bin green at a seven again \n",
      "Predicted: sla bree at ie no\n"
     ]
    }
   ],
   "source": [
    "import jiwer\n",
    "\n",
    "def calculate_wer(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Calculate the Word Error Rate (WER).\n",
    "    - reference: The ground truth string.\n",
    "    - hypothesis: The predicted string.\n",
    "    Returns the WER as a float.\n",
    "    \"\"\"\n",
    "    return jiwer.wer(reference, hypothesis)\n",
    "\n",
    "def calculate_cer(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Calculate the Character Error Rate (CER).\n",
    "    - reference: The ground truth string.\n",
    "    - hypothesis: The predicted string.\n",
    "    Returns the CER as a float.\n",
    "    \"\"\"\n",
    "    return jiwer.cer(reference, hypothesis)\n",
    "\n",
    "def ctc_greedy_decoder(output, int_to_char, blank_label):\n",
    "    \"\"\"\n",
    "    Decodes the output of the network using a greedy approach.\n",
    "    - output: The raw output from the network.\n",
    "    - int2char: A dictionary mapping indices to characters.\n",
    "    - blank_label: The index of the blank label.\n",
    "    Returns a list of decoded words.\n",
    "    \"\"\"\n",
    "    decoded_words = []\n",
    "    for batch in output:\n",
    "        word = []\n",
    "        prev_char = None\n",
    "        for i in batch:\n",
    "            char_idx = i.item()\n",
    "            if char_idx != blank_label and (prev_char is None or char_idx != prev_char):\n",
    "                word.append(int_to_char(char_idx))\n",
    "            prev_char = char_idx\n",
    "        decoded_words.append(''.join(word))\n",
    "    return decoded_words\n",
    "\n",
    "# Ensure the blank label is mapped to a space character if needed\n",
    "# int2char[blank_label] = ' '\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_val_loss = 0\n",
    "    all_golden_words = []\n",
    "    all_predicted_words = []\n",
    "    for sequences, labels in test_loader:\n",
    "        sequences = sequences.view(sequences.size(0), sequences.size(1), -1)  # Flatten the input dimensions\n",
    "        \n",
    "        # Print the shape of sequences\n",
    "        print(f'Validation Batch, sequences shape: {sequences.shape}')\n",
    "        \n",
    "        # Move tensors to the appropriate device\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(sequences)\n",
    "        \n",
    "        # Convert outputs to predicted indices\n",
    "        max_indices = torch.argmax(outputs, dim=2)\n",
    "        \n",
    "        # Decode the predicted indices to words\n",
    "        predicted_words = ctc_greedy_decoder(max_indices, int_to_char, 0)\n",
    "        \n",
    "        # Convert golden labels to characters\n",
    "        golden_words = []\n",
    "        for label in labels:\n",
    "            word = []\n",
    "            for i in label:\n",
    "                if i.item() != 0:  # Ignore the padding (blank) label\n",
    "                    word.append(int_to_char(i.item()))\n",
    "            golden_words.append(''.join(word))\n",
    "        \n",
    "        # Collect all golden and predicted words\n",
    "        all_golden_words.extend(golden_words)\n",
    "        all_predicted_words.extend(predicted_words)\n",
    "        \n",
    "        # Print the golden and predicted labels\n",
    "        for i in range(len(golden_words)):\n",
    "            print(f\"Golden: {golden_words[i]}\")\n",
    "            print(f\"Predicted: {predicted_words[i]}\")\n",
    "            \n",
    "        # Only process the first batch for printing\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
