{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : CTC Loss\n",
    "from typing import List\n",
    "\n",
    "import cv2\n",
    "import gdown\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "\n",
    "#import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('data.npy', data)\n",
    "# np.save('labels.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(\"face_landmarks_data3.csv\", index_col=0)\n",
    "df4 = pd.read_csv(\"face_landmarks_data4.csv\", index_col=0)\n",
    "df5 = pd.read_csv(\"face_landmarks_data5.csv\", index_col=0)\n",
    "df = pd.concat([df3, df4, df5], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path_counts = df['video_path'].value_counts()\n",
    "\n",
    "video_paths_to_keep = video_path_counts[video_path_counts == 3000].index\n",
    "\n",
    "df = df[df['video_path'].isin(video_paths_to_keep)]\n",
    "\n",
    "df_filtered = df.copy()\n",
    "chunks = []\n",
    "num_frames_per_chunk = 3000\n",
    "for video_path, group in df_filtered.groupby('video_path'):\n",
    "    num_frames = len(group)\n",
    "    num_chunks = num_frames // num_frames_per_chunk\n",
    "    for i in range(num_chunks):\n",
    "        chunk = group.iloc[i*num_frames_per_chunk:(i+1)*num_frames_per_chunk]\n",
    "        chunk_reshaped = chunk[['x', 'y', 'z']].values.reshape(-1, 75, 40*3)\n",
    "        chunks.append(chunk_reshaped)\n",
    "\n",
    "input_data = np.concatenate(chunks, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2977, 75, 120)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab():\n",
    "    vocab = \"abcdefghijklmnopqrstuvwxyz123456789 \"\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def char_to_int(char):\n",
    "    # shift 1 \n",
    "    vocab = \"abcdefghijklmnopqrstuvwxyz123456789 \"\n",
    "    return vocab.index(char) + 1 if char in vocab else -1\n",
    "\n",
    "def int_to_char(index):\n",
    "    # shift 1 \n",
    "    vocab = \"abcdefghijklmnopqrstuvwxyz123456789 \"\n",
    "    return vocab[index - 1] if 1 <= index <= len(vocab) else ''\n",
    "\n",
    "def load_alignments(path:str) -> List[str]:\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    tokens = []\n",
    "    for line in lines:\n",
    "        line = line.split()\n",
    "        if line[2] != 'sil':\n",
    "            tokens.extend([*line[2]])\n",
    "            tokens.append(' ')\n",
    "    return [char_to_int(token) for token in tokens]\n",
    "\n",
    "all_alignments = []\n",
    "for video_path in df['video_path'].unique():\n",
    "    datapath = video_path.split('/')[0]\n",
    "    speaker_path = video_path.split('/')[-1].split('\\\\')[0]\n",
    "    vid_path = video_path.split('/')[-1].split('\\\\')[-1].split('.')[0]\n",
    "    \n",
    "    alignment_path = os.path.join(f'{datapath}','align',f'{vid_path}.align')\n",
    "    alignments = load_alignments(alignment_path) \n",
    "    all_alignments.append(alignments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(len(label) for label in all_alignments)\n",
    "label_data = [np.pad(label, (0, max_len - len(label)), 'constant', constant_values=0) for label in all_alignments]\n",
    "\n",
    "# Convert label_data to numpy array\n",
    "label_data = np.array(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  9, 14, ...,  0,  0,  0],\n",
       "       [ 2,  9, 14, ...,  0,  0,  0],\n",
       "       [ 2,  9, 14, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [19,  5, 20, ...,  0,  0,  0],\n",
       "       [19,  5, 20, ...,  0,  0,  0],\n",
       "       [19,  5, 20, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2977, 75, 120])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data_tensor = torch.tensor(input_data, dtype=torch.float32)\n",
    "input_data_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data_tensor = torch.tensor(label_data, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(input_data_tensor, label_data_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = create_vocab()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Training Loss: 3.4547, Validation Loss: 2.8069\n",
      "Epoch [2/100], Training Loss: 2.7178, Validation Loss: 2.6307\n",
      "Epoch [3/100], Training Loss: 2.5199, Validation Loss: 2.4286\n",
      "Epoch [4/100], Training Loss: 2.3810, Validation Loss: 2.2826\n",
      "Epoch [5/100], Training Loss: 2.2572, Validation Loss: 2.1659\n",
      "Epoch [6/100], Training Loss: 2.1302, Validation Loss: 2.0338\n",
      "Epoch [7/100], Training Loss: 2.0233, Validation Loss: 1.9185\n",
      "Epoch [8/100], Training Loss: 1.9403, Validation Loss: 1.8252\n",
      "Epoch [9/100], Training Loss: 1.8753, Validation Loss: 1.7709\n",
      "Epoch [10/100], Training Loss: 1.8223, Validation Loss: 1.7115\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 68>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     80\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mlog_softmax(\u001b[38;5;241m2\u001b[39m), labels, input_lengths, target_lengths)\n\u001b[0;32m     82\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 83\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     86\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "class LipReadingRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LipReadingRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_size)\n",
    "        )\n",
    "        \n",
    "        self.bilstm = nn.LSTM(hidden_size, hidden_size, num_layers=3, batch_first=True, bidirectional=True, dropout=0.3)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for layer in self.input_layer:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "        \n",
    "        for name, param in self.bilstm.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "                \n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)  \n",
    "        h0 = torch.zeros(6, x.size(0), self.hidden_size).to(x.device)  # 6 for 3 layers bidirectional\n",
    "        c0 = torch.zeros(6, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.bilstm(x, (h0, c0))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)  \n",
    "        \n",
    "        return out\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "input_size = 120\n",
    "hidden_size = 256\n",
    "num_classes = len(vocab) + 1\n",
    "model = LipReadingRNN(input_size, hidden_size, num_classes).to(device)\n",
    "criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = StepLR(optimizer, step_size=200, gamma=0.1)\n",
    "\n",
    "\n",
    "num_epochs = 100\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for sequences, labels in train_loader:\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(sequences)\n",
    "        \n",
    "        outputs = outputs.permute(1, 0, 2)\n",
    "        input_lengths = torch.full((sequences.size(0),), sequences.size(1), dtype=torch.long)\n",
    "        target_lengths = torch.tensor([label[label != 0].size(0) for label in labels], dtype=torch.long)\n",
    "        loss = criterion(outputs.log_softmax(2), labels, input_lengths, target_lengths)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in test_loader:\n",
    "            sequences = sequences.to(device)\n",
    "            labels = labels.to(device)\n",
    "            sequences = sequences.view(sequences.size(0), sequences.size(1), -1) # (batch_size, seq_len, input_size)\n",
    "\n",
    "            outputs = model(sequences)\n",
    "\n",
    "            outputs = outputs.permute(1, 0, 2)\n",
    "            input_lengths = torch.full((sequences.size(0),), sequences.size(1), dtype=torch.long)\n",
    "            target_lengths = torch.tensor([label[label != 0].size(0) for label in labels], dtype=torch.long)\n",
    "            loss = criterion(outputs.log_softmax(2), labels, input_lengths, target_lengths)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {total_loss/len(train_loader):.4f}, Validation Loss: {val_loss/len(test_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch, sequences shape: torch.Size([32, 75, 120])\n",
      "Golden: lay white in e zero now \n",
      "Predicted: le re it  n \n",
      "Golden: place white with k three please \n",
      "Predicted: le re it  n \n",
      "Golden: place white with q seven again \n",
      "Predicted: le re it  n \n",
      "Golden: lay white by l five please \n",
      "Predicted: le re i  n \n",
      "Golden: set red at b one again \n",
      "Predicted: le re it  n \n",
      "Golden: set blue by a nine again \n",
      "Predicted: le re it  n \n",
      "Golden: lay blue by k two soon \n",
      "Predicted: le re i  n \n",
      "Golden: place blue by p one again \n",
      "Predicted: le re it  n \n",
      "Golden: bin white by a two please \n",
      "Predicted: le re it  n \n",
      "Golden: bin blue in e five now \n",
      "Predicted: le re i  n \n",
      "Golden: lay red in q six please \n",
      "Predicted: le re it  n \n",
      "Golden: bin green by u two again \n",
      "Predicted: le re i  n \n",
      "Golden: bin green in g six please \n",
      "Predicted: le re it  n \n",
      "Golden: set green with j one soon \n",
      "Predicted: le re it  n \n",
      "Golden: bin red by m six again \n",
      "Predicted: le re it  n \n",
      "Golden: set red at n four now \n",
      "Predicted: le re it  n \n",
      "Golden: place red in v one now \n",
      "Predicted: le re i  n \n",
      "Golden: bin red at m one soon \n",
      "Predicted: le re it  n \n",
      "Golden: set blue at a two please \n",
      "Predicted: le re it  n \n",
      "Golden: place red at c six please \n",
      "Predicted: le re it  n \n",
      "Golden: place blue with j three again \n",
      "Predicted: le re it  n \n",
      "Golden: place green in j six please \n",
      "Predicted: le re it  n \n",
      "Golden: lay white by z sp two now \n",
      "Predicted: le re it  n \n",
      "Golden: bin blue by s one now \n",
      "Predicted: le re i  n \n",
      "Golden: lay green in y nine now \n",
      "Predicted: le re i  n \n",
      "Golden: set green at v three please \n",
      "Predicted: le re it  n \n",
      "Golden: lay white in k six now \n",
      "Predicted: le re it  n \n",
      "Golden: bin green with u four soon \n",
      "Predicted: le re i  n \n",
      "Golden: set red at n seven please \n",
      "Predicted: le re i  n \n",
      "Golden: bin blue with s nine again \n",
      "Predicted: le re it  n \n",
      "Golden: lay white at l zero soon \n",
      "Predicted: le re it  n \n",
      "Golden: lay green with m seven again \n",
      "Predicted: le re it  n \n"
     ]
    }
   ],
   "source": [
    "import jiwer\n",
    "\n",
    "def calculate_wer(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Calculate the Word Error Rate (WER).\n",
    "    - reference: The ground truth string.\n",
    "    - hypothesis: The predicted string.\n",
    "    Returns the WER as a float.\n",
    "    \"\"\"\n",
    "    return jiwer.wer(reference, hypothesis)\n",
    "\n",
    "def calculate_cer(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Calculate the Character Error Rate (CER).\n",
    "    - reference: The ground truth string.\n",
    "    - hypothesis: The predicted string.\n",
    "    Returns the CER as a float.\n",
    "    \"\"\"\n",
    "    return jiwer.cer(reference, hypothesis)\n",
    "\n",
    "def ctc_greedy_decoder(output, int_to_char, blank_label):\n",
    "    \"\"\"\n",
    "    Decodes the output of the network using a greedy approach.\n",
    "    - output: The raw output from the network.\n",
    "    - int2char: A dictionary mapping indices to characters.\n",
    "    - blank_label: The index of the blank label.\n",
    "    Returns a list of decoded words.\n",
    "    \"\"\"\n",
    "    decoded_words = []\n",
    "    for batch in output:\n",
    "        word = []\n",
    "        prev_char = None\n",
    "        for i in batch:\n",
    "            char_idx = i.item()\n",
    "            if char_idx != blank_label and (prev_char is None or char_idx != prev_char):\n",
    "                word.append(int_to_char(char_idx))\n",
    "            prev_char = char_idx\n",
    "        decoded_words.append(''.join(word))\n",
    "    return decoded_words\n",
    "\n",
    "# Ensure the blank label is mapped to a space character if needed\n",
    "# int2char[blank_label] = ' '\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_val_loss = 0\n",
    "    all_golden_words = []\n",
    "    all_predicted_words = []\n",
    "    for sequences, labels in test_loader:\n",
    "        sequences = sequences.view(sequences.size(0), sequences.size(1), -1)  # Flatten the input dimensions\n",
    "        \n",
    "        # Print the shape of sequences\n",
    "        print(f'Validation Batch, sequences shape: {sequences.shape}')\n",
    "        \n",
    "        # Move tensors to the appropriate device\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(sequences)\n",
    "        \n",
    "        # Convert outputs to predicted indices\n",
    "        max_indices = torch.argmax(outputs, dim=2)\n",
    "        \n",
    "        # Decode the predicted indices to words\n",
    "        predicted_words = ctc_greedy_decoder(max_indices, int_to_char, 0)\n",
    "        \n",
    "        # Convert golden labels to characters\n",
    "        golden_words = []\n",
    "        for label in labels:\n",
    "            word = []\n",
    "            for i in label:\n",
    "                if i.item() != 0:  # Ignore the padding (blank) label\n",
    "                    word.append(int_to_char(i.item()))\n",
    "            golden_words.append(''.join(word))\n",
    "        \n",
    "        # Collect all golden and predicted words\n",
    "        all_golden_words.extend(golden_words)\n",
    "        all_predicted_words.extend(predicted_words)\n",
    "        \n",
    "        # Print the golden and predicted labels\n",
    "        for i in range(len(golden_words)):\n",
    "            print(f\"Golden: {golden_words[i]}\")\n",
    "            print(f\"Predicted: {predicted_words[i]}\")\n",
    "            \n",
    "        # Only process the first batch for printing\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
