{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import cv2\n",
    "import gdown\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "\n",
    "#import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook lip_keypoint_crossentropy.ipynb to python\n",
      "[NbConvertApp] Writing 9858 bytes to lip_keypoint_crossentropy.py\n"
     ]
    }
   ],
   "source": [
    "# !jupyter nbconvert lip_keypoint_crossentropy.ipynb --to python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(\"face_landmarks_data3.csv\", index_col=0)\n",
    "df4 = pd.read_csv(\"face_landmarks_data4.csv\", index_col=0)\n",
    "df5 = pd.read_csv(\"face_landmarks_data5.csv\", index_col=0)\n",
    "df6 = pd.read_csv(\"face_landmarks_data6.csv\", index_col=0)\n",
    "df = pd.concat([df3, df4, df5, df6], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path_counts = df['video_path'].value_counts()\n",
    "\n",
    "video_paths_to_keep = video_path_counts[video_path_counts == 3000].index\n",
    "\n",
    "df = df[df['video_path'].isin(video_paths_to_keep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>visibility</th>\n",
       "      <th>video_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.528295</td>\n",
       "      <td>0.712854</td>\n",
       "      <td>-0.031539</td>\n",
       "      <td>0.0</td>\n",
       "      <td>data3/s3\\bbaf1s.mpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.543726</td>\n",
       "      <td>0.710494</td>\n",
       "      <td>-0.030296</td>\n",
       "      <td>0.0</td>\n",
       "      <td>data3/s3\\bbaf1s.mpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.559562</td>\n",
       "      <td>0.715571</td>\n",
       "      <td>-0.022568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>data3/s3\\bbaf1s.mpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.570321</td>\n",
       "      <td>0.721982</td>\n",
       "      <td>-0.012750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>data3/s3\\bbaf1s.mpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.528245</td>\n",
       "      <td>0.728252</td>\n",
       "      <td>-0.018017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>data3/s3\\bbaf1s.mpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9008875</th>\n",
       "      <td>75</td>\n",
       "      <td>0.511908</td>\n",
       "      <td>0.742837</td>\n",
       "      <td>-0.020150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>data6/s6\\bbal1n.mpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9008876</th>\n",
       "      <td>75</td>\n",
       "      <td>0.489571</td>\n",
       "      <td>0.741860</td>\n",
       "      <td>-0.010639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>data6/s6\\bbal1n.mpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9008877</th>\n",
       "      <td>75</td>\n",
       "      <td>0.482818</td>\n",
       "      <td>0.756543</td>\n",
       "      <td>-0.012075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>data6/s6\\bbal1n.mpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9008878</th>\n",
       "      <td>75</td>\n",
       "      <td>0.482979</td>\n",
       "      <td>0.741705</td>\n",
       "      <td>-0.004377</td>\n",
       "      <td>0.0</td>\n",
       "      <td>data6/s6\\bbal1n.mpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9008879</th>\n",
       "      <td>75</td>\n",
       "      <td>0.572588</td>\n",
       "      <td>0.743352</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>data6/s6\\bbal1n.mpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8946000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         frame         x         y         z  visibility           video_path\n",
       "0            1  0.528295  0.712854 -0.031539         0.0  data3/s3\\bbaf1s.mpg\n",
       "1            1  0.543726  0.710494 -0.030296         0.0  data3/s3\\bbaf1s.mpg\n",
       "2            1  0.559562  0.715571 -0.022568         0.0  data3/s3\\bbaf1s.mpg\n",
       "3            1  0.570321  0.721982 -0.012750         0.0  data3/s3\\bbaf1s.mpg\n",
       "4            1  0.528245  0.728252 -0.018017         0.0  data3/s3\\bbaf1s.mpg\n",
       "...        ...       ...       ...       ...         ...                  ...\n",
       "9008875     75  0.511908  0.742837 -0.020150         0.0  data6/s6\\bbal1n.mpg\n",
       "9008876     75  0.489571  0.741860 -0.010639         0.0  data6/s6\\bbal1n.mpg\n",
       "9008877     75  0.482818  0.756543 -0.012075         0.0  data6/s6\\bbal1n.mpg\n",
       "9008878     75  0.482979  0.741705 -0.004377         0.0  data6/s6\\bbal1n.mpg\n",
       "9008879     75  0.572588  0.743352  0.002281         0.0  data6/s6\\bbal1n.mpg\n",
       "\n",
       "[8946000 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_x = np.mean(df['x'])\n",
    "std_x = np.std(df['x'])\n",
    "mean_y = np.mean(df['y'])\n",
    "std_y = np.std(df['y'])\n",
    "mean_z = np.mean(df['z'])\n",
    "std_z = np.std(df['z'])\n",
    "\n",
    "df['x'] = (df['x'] - mean_x) / std_x\n",
    "df['y'] = (df['y'] - mean_y) / std_y\n",
    "df['z'] = (df['z'] - mean_z) / std_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>visibility</th>\n",
       "      <th>video_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.427591</td>\n",
       "      <td>-0.291086</td>\n",
       "      <td>-1.585347</td>\n",
       "      <td>0.0</td>\n",
       "      <td>data3/s3\\bbaf1s.mpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.795735</td>\n",
       "      <td>-0.335998</td>\n",
       "      <td>-1.484981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>data3/s3\\bbaf1s.mpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1.173533</td>\n",
       "      <td>-0.239353</td>\n",
       "      <td>-0.861128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>data3/s3\\bbaf1s.mpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.430211</td>\n",
       "      <td>-0.117326</td>\n",
       "      <td>-0.068493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>data3/s3\\bbaf1s.mpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.426400</td>\n",
       "      <td>0.002025</td>\n",
       "      <td>-0.493689</td>\n",
       "      <td>0.0</td>\n",
       "      <td>data3/s3\\bbaf1s.mpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   frame         x         y         z  visibility           video_path\n",
       "0      1  0.427591 -0.291086 -1.585347         0.0  data3/s3\\bbaf1s.mpg\n",
       "1      1  0.795735 -0.335998 -1.484981         0.0  data3/s3\\bbaf1s.mpg\n",
       "2      1  1.173533 -0.239353 -0.861128         0.0  data3/s3\\bbaf1s.mpg\n",
       "3      1  1.430211 -0.117326 -0.068493         0.0  data3/s3\\bbaf1s.mpg\n",
       "4      1  0.426400  0.002025 -0.493689         0.0  data3/s3\\bbaf1s.mpg"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>visibility</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video_path</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data3/s3\\bbaf1s.mpg</th>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data3/s3\\bbaf2p.mpg</th>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data3/s3\\bbaf3a.mpg</th>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data3/s3\\bbafzn.mpg</th>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data3/s3\\bbal4n.mpg</th>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data6/s6\\swwpza.mpg</th>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data6/s6\\swwv1n.mpg</th>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data6/s6\\swwv2s.mpg</th>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data6/s6\\swwv3p.mpg</th>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data6/s6\\swwv4a.mpg</th>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3967 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     frame     x     y     z  visibility\n",
       "video_path                                              \n",
       "data3/s3\\bbaf1s.mpg   3000  3000  3000  3000        3000\n",
       "data3/s3\\bbaf2p.mpg   3000  3000  3000  3000        3000\n",
       "data3/s3\\bbaf3a.mpg   3000  3000  3000  3000        3000\n",
       "data3/s3\\bbafzn.mpg   3000  3000  3000  3000        3000\n",
       "data3/s3\\bbal4n.mpg   3000  3000  3000  3000        3000\n",
       "...                    ...   ...   ...   ...         ...\n",
       "data6/s6\\swwpza.mpg   3000  3000  3000  3000        3000\n",
       "data6/s6\\swwv1n.mpg   3000  3000  3000  3000        3000\n",
       "data6/s6\\swwv2s.mpg   3000  3000  3000  3000        3000\n",
       "data6/s6\\swwv3p.mpg   3000  3000  3000  3000        3000\n",
       "data6/s6\\swwv4a.mpg   3000  3000  3000  3000        3000\n",
       "\n",
       "[3967 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('video_path').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df.copy()\n",
    "chunks = []\n",
    "num_frames_per_chunk = 3000\n",
    "for video_path, group in df_filtered.groupby('video_path'):\n",
    "    num_frames = len(group)\n",
    "    num_chunks = num_frames // num_frames_per_chunk\n",
    "    for i in range(num_chunks):\n",
    "        chunk = group.iloc[i*num_frames_per_chunk:(i+1)*num_frames_per_chunk]\n",
    "        chunk_reshaped = chunk[['x', 'y', 'z']].values.reshape(-1, 75, 40*3)\n",
    "        chunks.append(chunk_reshaped)\n",
    "\n",
    "input_data = np.concatenate(chunks, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 120)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab():\n",
    "    vocab = \"abcdefghijklmnopqrstuvwxyz123456789 \"\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def char_to_int(char):\n",
    "    # shift 1 \n",
    "    vocab = \"abcdefghijklmnopqrstuvwxyz123456789 \"\n",
    "    return vocab.index(char) + 1 if char in vocab else -1\n",
    "\n",
    "def int_to_char(index):\n",
    "    # shift 1 \n",
    "    vocab = \"abcdefghijklmnopqrstuvwxyz123456789 \"\n",
    "    return vocab[index - 1] if 1 <= index <= len(vocab) else ''\n",
    "\n",
    "def load_alignments(path:str) -> List[str]:\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    tokens = []\n",
    "    for line in lines:\n",
    "        line = line.split()\n",
    "        if line[2] != 'sil':\n",
    "            tokens.extend([*line[2]])\n",
    "            tokens.append(' ')\n",
    "    return [char_to_int(token) for token in tokens]\n",
    "\n",
    "all_alignments = []\n",
    "for video_path in df['video_path'].unique():\n",
    "    datapath = video_path.split('/')[0]\n",
    "    speaker_path = video_path.split('/')[-1].split('\\\\')[0]\n",
    "    vid_path = video_path.split('/')[-1].split('\\\\')[-1].split('.')[0]\n",
    "    \n",
    "    alignment_path = os.path.join(f'{datapath}','align',f'{vid_path}.align')\n",
    "    alignments = load_alignments(alignment_path) \n",
    "    all_alignments.append(alignments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2982"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_path = df['video_path'].iloc[0]\n",
    "# datapath = video_path.split('/')\n",
    "# speakert_path = video_path.split('/')[-1].split('\\\\')[0]\n",
    "# vid_path = video_path.split('/')[-1].split('\\\\')[-1].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (2982, 75, 120)\n",
      "Number of label sequences: 2982\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label_data = [np.array(label) for label in all_alignments if label]\n",
    "print(\"Input data shape:\", input_data.shape)\n",
    "print(\"Number of label sequences:\", len(label_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LipReadingDataset(Dataset):\n",
    "    def __init__(self, input_features, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_features (numpy array): Input features with shape (984, 1, 75, 80)\n",
    "            labels (numpy array): Labels with shape (984, 30)\n",
    "        \"\"\"\n",
    "        self.input_features = input_features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert numpy arrays to torch tensors\n",
    "        video_frames = torch.from_numpy(self.input_features[idx]).float() \n",
    "        character_labels = torch.from_numpy(self.labels[idx]).long()\n",
    "        return video_frames, character_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2982, 75, 120)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "\n",
    "    inputs_tensor = [torch.tensor(input).float() for input in inputs]\n",
    "    targets_tensor = [torch.tensor(target).long() for target in targets]\n",
    "\n",
    "    # max len\n",
    "    input_lengths = torch.tensor([len(input) for input in inputs_tensor], dtype=torch.long)\n",
    "    target_lengths = torch.tensor([len(target) for target in targets_tensor], dtype=torch.long)\n",
    "    max_length = max(max([len(input) for input in inputs_tensor]), 75)\n",
    "    # padding\n",
    "    inputs_padded = pad_sequence(inputs_tensor, batch_first=True, padding_value=0) \n",
    "    targets_padded = pad_sequence([torch.cat([target, torch.tensor([-1] * (max_length - len(target)))] if len(target) < max_length else target) for target in targets_tensor], batch_first=True, padding_value=-1)\n",
    "\n",
    "    return inputs_padded, targets_padded, input_lengths, target_lengths\n",
    "\n",
    "dataset = LipReadingDataset(input_data, label_data)\n",
    "\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "test_size = total_size - train_size\n",
    "\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_37384\\3458008796.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs_tensor = [torch.tensor(input).float() for input in inputs]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_37384\\3458008796.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets_tensor = [torch.tensor(target).long() for target in targets]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 4.6673e-01,  7.1324e-01, -3.3092e-02,  ...,  5.1191e-01,\n",
       "            7.4153e-01,  1.8019e-03],\n",
       "          [ 4.6870e-01,  7.1356e-01, -3.3321e-02,  ...,  5.1306e-01,\n",
       "            7.4056e-01,  1.2250e-03],\n",
       "          [ 4.6821e-01,  7.1345e-01, -3.3636e-02,  ...,  5.1286e-01,\n",
       "            7.4119e-01,  5.9725e-04],\n",
       "          ...,\n",
       "          [ 4.7122e-01,  7.0895e-01, -3.4431e-02,  ...,  5.1533e-01,\n",
       "            7.3546e-01, -1.8254e-03],\n",
       "          [ 4.7153e-01,  7.0848e-01, -3.4342e-02,  ...,  5.1584e-01,\n",
       "            7.3475e-01, -2.1324e-03],\n",
       "          [ 4.7228e-01,  7.0780e-01, -3.4673e-02,  ...,  5.1635e-01,\n",
       "            7.3460e-01, -2.0181e-03]],\n",
       " \n",
       "         [[ 4.6164e-01,  7.1405e-01, -3.3575e-02,  ...,  5.0821e-01,\n",
       "            7.3560e-01, -2.6842e-03],\n",
       "          [ 4.6331e-01,  7.1494e-01, -3.3346e-02,  ...,  5.1006e-01,\n",
       "            7.3519e-01, -1.1624e-03],\n",
       "          [ 4.6300e-01,  7.1422e-01, -3.3552e-02,  ...,  5.0860e-01,\n",
       "            7.3693e-01, -1.5794e-03],\n",
       "          ...,\n",
       "          [ 4.7077e-01,  7.1477e-01, -3.0620e-02,  ...,  5.1835e-01,\n",
       "            7.3132e-01,  3.8927e-03],\n",
       "          [ 4.7114e-01,  7.1440e-01, -3.1011e-02,  ...,  5.1862e-01,\n",
       "            7.3101e-01,  3.1661e-03],\n",
       "          [ 4.7058e-01,  7.1439e-01, -3.0924e-02,  ...,  5.1824e-01,\n",
       "            7.3122e-01,  3.3577e-03]],\n",
       " \n",
       "         [[ 5.3866e-01,  7.7742e-01, -3.6526e-02,  ...,  5.8902e-01,\n",
       "            8.0768e-01,  5.7868e-04],\n",
       "          [ 5.3946e-01,  7.7954e-01, -4.0614e-02,  ...,  5.9103e-01,\n",
       "            8.1191e-01, -9.7812e-04],\n",
       "          [ 5.3762e-01,  7.7862e-01, -4.0126e-02,  ...,  5.8896e-01,\n",
       "            8.1193e-01, -1.9288e-03],\n",
       "          ...,\n",
       "          [ 5.4038e-01,  7.7989e-01, -3.8005e-02,  ...,  5.9337e-01,\n",
       "            8.0251e-01,  1.8114e-04],\n",
       "          [ 5.4051e-01,  7.8116e-01, -3.7750e-02,  ...,  5.9373e-01,\n",
       "            8.0384e-01,  8.7713e-04],\n",
       "          [ 5.4028e-01,  7.8155e-01, -3.7605e-02,  ...,  5.9342e-01,\n",
       "            8.0419e-01,  8.5778e-04]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 5.0297e-01,  7.2337e-01, -4.5820e-02,  ...,  5.5716e-01,\n",
       "            7.6698e-01, -4.7926e-03],\n",
       "          [ 5.0357e-01,  7.2221e-01, -4.3661e-02,  ...,  5.5977e-01,\n",
       "            7.6446e-01,  1.4687e-03],\n",
       "          [ 5.0532e-01,  7.2208e-01, -4.4152e-02,  ...,  5.6068e-01,\n",
       "            7.6318e-01,  4.2721e-04],\n",
       "          ...,\n",
       "          [ 5.0007e-01,  7.3229e-01, -4.0823e-02,  ...,  5.5803e-01,\n",
       "            7.5808e-01,  4.5171e-03],\n",
       "          [ 5.0010e-01,  7.3260e-01, -4.0667e-02,  ...,  5.5781e-01,\n",
       "            7.5818e-01,  4.5988e-03],\n",
       "          [ 5.0054e-01,  7.3264e-01, -4.0417e-02,  ...,  5.5829e-01,\n",
       "            7.5771e-01,  5.2188e-03]],\n",
       " \n",
       "         [[ 5.0682e-01,  7.2381e-01, -4.1349e-02,  ...,  5.6020e-01,\n",
       "            7.4844e-01, -3.5657e-03],\n",
       "          [ 5.0976e-01,  7.2944e-01, -4.0219e-02,  ...,  5.6238e-01,\n",
       "            7.5461e-01, -3.1493e-03],\n",
       "          [ 5.0776e-01,  7.2873e-01, -4.0732e-02,  ...,  5.6048e-01,\n",
       "            7.5469e-01, -4.8228e-03],\n",
       "          ...,\n",
       "          [ 5.2201e-01,  7.3787e-01, -3.8666e-02,  ...,  5.7323e-01,\n",
       "            7.6165e-01,  5.7295e-04],\n",
       "          [ 5.2157e-01,  7.3728e-01, -3.8933e-02,  ...,  5.7269e-01,\n",
       "            7.6153e-01, -2.3850e-06],\n",
       "          [ 5.2166e-01,  7.3753e-01, -3.8880e-02,  ...,  5.7273e-01,\n",
       "            7.6163e-01,  2.1218e-04]],\n",
       " \n",
       "         [[ 4.6741e-01,  7.8321e-01, -3.1962e-02,  ...,  5.1931e-01,\n",
       "            8.1139e-01,  1.1141e-02],\n",
       "          [ 4.7023e-01,  7.8853e-01, -3.6782e-02,  ...,  5.2297e-01,\n",
       "            8.2180e-01,  1.0069e-02],\n",
       "          [ 4.7173e-01,  7.9120e-01, -3.8074e-02,  ...,  5.2302e-01,\n",
       "            8.2238e-01,  4.6676e-03],\n",
       "          ...,\n",
       "          [ 4.7339e-01,  7.9517e-01, -3.8079e-02,  ...,  5.2496e-01,\n",
       "            8.1857e-01,  2.9305e-03],\n",
       "          [ 4.7453e-01,  7.9522e-01, -3.7543e-02,  ...,  5.2620e-01,\n",
       "            8.1808e-01,  4.1311e-03],\n",
       "          [ 4.7448e-01,  7.9575e-01, -3.7529e-02,  ...,  5.2622e-01,\n",
       "            8.1857e-01,  3.9662e-03]]]),\n",
       " tensor([[16, 12,  1,  ..., -1, -1, -1],\n",
       "         [19,  5, 20,  ..., -1, -1, -1],\n",
       "         [12,  1, 25,  ..., -1, -1, -1],\n",
       "         ...,\n",
       "         [16, 12,  1,  ..., -1, -1, -1],\n",
       "         [12,  1, 25,  ..., -1, -1, -1],\n",
       "         [19,  5, 20,  ..., -1, -1, -1]]),\n",
       " tensor([75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75,\n",
       "         75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75]),\n",
       " tensor([29, 23, 26, 27, 25, 26, 27, 28, 25, 30, 24, 27, 23, 22, 24, 23, 25, 25,\n",
       "         27, 28, 26, 25, 27, 29, 24, 26, 27, 28, 27, 29, 24, 26]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LipReadingModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, num_lstm_layers=2, dropout_rate=0.2):\n",
    "        super(LipReadingModel, self).__init__()\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers=num_lstm_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x  # Shape: (batch_size, seq_len, num_classes)\n",
    "\n",
    "\n",
    "vocab = create_vocab()\n",
    "vocab_len = len(vocab)\n",
    "num_classes = vocab_len + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyz123456789 '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2982, 75, 120)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 120)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.28294802e-01,  7.12853551e-01, -3.15389112e-02, ...,\n",
       "         5.76892495e-01,  7.35325933e-01,  2.36975471e-03],\n",
       "       [ 5.29577672e-01,  7.11253285e-01, -3.28786299e-02, ...,\n",
       "         5.78541338e-01,  7.36495733e-01,  1.19938678e-03],\n",
       "       [ 5.29968917e-01,  7.12010741e-01, -3.30490768e-02, ...,\n",
       "         5.78672171e-01,  7.37926543e-01,  1.21604651e-03],\n",
       "       ...,\n",
       "       [ 5.26558101e-01,  7.04166114e-01, -3.39030139e-02, ...,\n",
       "         5.75963020e-01,  7.31105328e-01, -1.15881430e-03],\n",
       "       [ 5.26525080e-01,  7.05153167e-01, -3.42276581e-02, ...,\n",
       "         5.76362848e-01,  7.32087970e-01, -1.06557133e-03],\n",
       "       [ 5.26649058e-01,  7.05285549e-01, -3.37997042e-02, ...,\n",
       "         5.76122642e-01,  7.32028484e-01, -3.55579919e-04]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9048\\3458008796.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs_tensor = [torch.tensor(input).float() for input in inputs]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9048\\3458008796.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets_tensor = [torch.tensor(target).long() for target in targets]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 2.677615716457367, Validation Loss: 2.477437763214111\n",
      "Epoch 2, Training Loss: 2.343436715602875, Validation Loss: 2.2327731800079347\n",
      "Epoch 3, Training Loss: 2.195141487121582, Validation Loss: 2.14636435508728\n",
      "Epoch 4, Training Loss: 2.1290117168426512, Validation Loss: 2.0877509117126465\n",
      "Epoch 5, Training Loss: 2.095003356933594, Validation Loss: 2.073266191482544\n",
      "Epoch 6, Training Loss: 2.078658764362335, Validation Loss: 2.0635959339141845\n",
      "Epoch 7, Training Loss: 2.072378553152084, Validation Loss: 2.063204736709595\n",
      "Epoch 8, Training Loss: 2.068823471069336, Validation Loss: 2.055733962059021\n",
      "Epoch 9, Training Loss: 2.0569102287292482, Validation Loss: 2.0418472051620484\n",
      "Epoch 10, Training Loss: 2.0500200533866884, Validation Loss: 2.0361515426635743\n",
      "Epoch 11, Training Loss: 2.0456985306739806, Validation Loss: 2.045603322982788\n",
      "Epoch 12, Training Loss: 2.0434894037246703, Validation Loss: 2.0324061059951783\n",
      "Epoch 13, Training Loss: 2.0402947556972504, Validation Loss: 2.0342241430282595\n",
      "Epoch 14, Training Loss: 2.0391553556919098, Validation Loss: 2.0338217878341673\n",
      "Epoch 15, Training Loss: 2.03544402718544, Validation Loss: 2.030162754058838\n",
      "Epoch 16, Training Loss: 2.033377341032028, Validation Loss: 2.0299993467330935\n",
      "Epoch 17, Training Loss: 2.032193648815155, Validation Loss: 2.026463494300842\n",
      "Epoch 18, Training Loss: 2.0303588449954986, Validation Loss: 2.026046214103699\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 45>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     42\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, test_loader, criterion, optimizer, device, num_epochs)\u001b[0m\n\u001b[0;32m     13\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# (batch_size, num_classes, seq_len)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36mLipReadingModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layer(x)\n\u001b[1;32m---> 22\u001b[0m     x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(x)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:774\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    777\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    778\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, device, num_epochs=100):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for inputs, targets, input_lengths, target_lengths in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.transpose(1, 2)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets, input_lengths, target_lengths in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.transpose(1, 2)\n",
    "                val_loss = criterion(outputs, targets)\n",
    "                total_val_loss += val_loss.item()\n",
    "        avg_val_loss = total_val_loss / len(test_loader)\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model = LipReadingModel(input_dim=120, hidden_dim=256, num_classes=num_classes, num_lstm_layers=2, dropout_rate=0.5).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer, device, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LipReadingModel(\n",
       "  (input_layer): Sequential(\n",
       "    (0): Linear(in_features=120, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (lstm): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (output_layer): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=37, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_18936\\3458008796.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs_tensor = [torch.tensor(input).float() for input in inputs]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_18936\\3458008796.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets_tensor = [torch.tensor(target).long() for target in targets]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: lay green with m four again \n",
      "bin blue  t   nie  again     e e   \n",
      "original: place green by y two now \n",
      "pay white wn n nix nown        e   \n",
      "original: lay green by f six please \n",
      "let whiee t    eie   o             \n",
      "original: lay blue in x one soon \n",
      "bin blue it    eeee ooon     e  e  \n",
      "original: set red in t four soon \n",
      "bin e ien  n s sen  oow  i         \n",
      "original: set blue in m four now \n",
      "pla e den ty   ire soon e   e      \n",
      "original: lay blue in p nine please \n",
      "plt blue by h thre  noa e          \n",
      "original: lay green in z zero please \n",
      "pey bed   t    eee again           \n",
      "original: bin green with u three soon \n",
      "place blue  in j znre noo          \n",
      "original: lay red with r five please \n",
      "lay r den t    t nep please        \n",
      "original: bin red at s five again \n",
      "sey g ien  n z tou  aaan           \n",
      "original: bin white in g zero again \n",
      "lly r den ii   teo ngoin           \n",
      "original: bin white by g seven again \n",
      "bin glue by h t ne oon     aee     \n",
      "original: bin white in s five now \n",
      "bin e den it   noe soin     ease   \n",
      "original: bin white in m three again \n",
      "ply e uhut  it h sigen noo        e\n",
      "original: place blue by c three soon \n",
      "lay bede i t      oee ngaaain    e \n",
      "original: set red in a seven again \n",
      "lay blue by f ti e n e apaa e      \n",
      "original: lay white in k seven again \n",
      "lla e iee t     eoe  aon           \n",
      "original: lay red by y four soon \n",
      "pla e wee w    ze e  owase   se se \n",
      "original: lay green with g zero now \n",
      "ply whde       sight o oo s        \n",
      "original: bin green with o zero again \n",
      "lay redee  t    eooo again      e  \n",
      "original: set blue at g four soon \n",
      "pla e een iy s zive ooan n         \n",
      "original: lay white in r one please \n",
      "pey green i    tiv t agn           \n",
      "original: set green by p four please \n",
      "bin elue at t f  oe  oo         e  \n",
      "original: bin blue at l five please \n",
      "biy blue  i t   zee  ooin     e ee \n",
      "original: set blue by n three please \n",
      "bince ihn in o ren  owa    le  e   \n",
      "original: bin white in f five now \n",
      "bin g uen in   zeoe agwin          \n",
      "original: lay blue by d six soon \n",
      "pla eed n tn   t n  non  e  e  e   \n",
      "original: set white by i three soon \n",
      "say white wi h z ree nowase   e    \n",
      "original: lay red at e one soon \n",
      "pay beue t  t t re  gaain    aan   \n",
      "original: bin green with h six again \n",
      "pey white at n tiren no   see ee   \n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs_padded, targets_padded, input_lengths, target_lengths = next(iter(test_loader))\n",
    "\n",
    "    inputs_padded = inputs_padded.to(device)\n",
    "    \n",
    "    outputs = model(inputs_padded)\n",
    "    outputs = outputs.transpose(1, 2)\n",
    "\n",
    "    predicted_indices = outputs.argmax(dim=1)\n",
    "    for i in range(31):\n",
    "        print(\"original:\", ''.join(int_to_char(idx) for idx in targets_padded[i]))\n",
    "        print(\"\".join([int_to_char(index) for index in predicted_indices[i][:35]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
